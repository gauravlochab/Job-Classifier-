{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    "**Probelm Statement:** Make a classifier which takes in a job description and gives the department name for it.\n",
    "*   Use a neural network model\n",
    "*   Make use of a pre-trained Word Embeddings (example: Word2Vec, GloVe, etc.)\n",
    "*   Calculate the accuracy on a test set (data not used to train the model)\n",
    "\n",
    "**Problem Solving Approach:** \n",
    "_Provide a brief description of steps you followed for solving this problem_\n",
    "1. In part 1 - Text preprocessing , the first step was to extract the useful data in required format from given raw data in JSON format.  \n",
    "2. In part 2 - EDA - This step in for finding the class distribution of the data and other useful information about the data.\n",
    "3. In part 3 - Modelling and Evaluation - Different Models are trained on the given data including neural network and their accuracy is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Text Preprocessing\n",
    "\n",
    "_Include all text preprocesing steps like processing of json,csv files & data cleaning in this part._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import neccessary packages in below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for f in glob.glob(\"data/docs/*.json\"):\n",
    "    with open(f, \"rb\") as infile:\n",
    "        result.append(json.load(infile))\n",
    "\n",
    "type(result)\n",
    "\n",
    "columns = ['Document ID','job_industry','Company_des','job_description',\n",
    "         'Job_Department','Industry','Other_Skills','Skills','Location','Keywords'\n",
    "           ,'job_title']\n",
    "df = pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(result)):\n",
    "    df.at[i, 'Document ID']              =         result[i].get('_id')\n",
    "    df.at[i, 'job_industry']    =        (result[i].get('api_data')).get('job_industry')\n",
    "    df.at[i, 'Company_des']     =        (result[i].get('company_info')).get('Company Description')\n",
    "    df.at[i, 'Industry']        =        (result[i].get('other_details')).get('Industry:')\n",
    "    df.at[i, 'job_description'] =        (result[i].get('jd_information')).get('description')\n",
    "    df.at[i, 'Other_Skills']    =        (result[i].get('other_details')).get('Other Skills:') \n",
    "    df.at[i, 'Skills']    =        (result[i].get('other_details')).get('Skills:') \n",
    "    df.at[i, 'Job_Department']      =    (result[i].get('other_details')).get('Department:')\n",
    "    df.at[i, 'Location']        =        (result[i].get('api_data')).get('job_location')\n",
    "    df.at[i, 'Keywords']        =        (result[i].get('api_data')).get('job_keywords')\n",
    "    df.at[i, 'job_title']       =        (result[i].get('api_data')).get('job_title')\n",
    "\n",
    "df.sort_values('Document ID')\n",
    "    \n",
    "df.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = pd.read_csv('data/document_departments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keys.sort_values('Document ID')\n",
    "keys.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame()\n",
    "dataframe = pd.concat([df,keys], axis=1)\n",
    "dataframe = dataframe.drop('Document ID', axis=1)\n",
    "dataframe['Document ID'] = keys['Document ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Exploratoty Data Analysis\n",
    "\n",
    "_Include EDA steps like finding distribution of Departments in this part, you may also use plots for EDA._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Department'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the classes that are scarce \n",
    "the_list = ['Safety ','Learning and Development ','Procurement','Flight Operations ','Marine Deck ','Airline Ground Operations','QA ','Marine Service Steward ','Logistics','Flight Operations','Data entry','Marine Engineering ']\n",
    "\n",
    "for i in range(1162):\n",
    "    if (dataframe.at[i,'Department']  in the_list):\n",
    "        dataframe = dataframe.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Department'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.iloc[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_list = ['Public Relations ','Content','Recruitment','Engineering Design Construction']\n",
    "rows_list = []\n",
    "val_rep = 80\n",
    "cnt=0\n",
    "for i in range(1139):\n",
    "    if (dataframe.at[i,'Department']  in sampling_list):\n",
    "           for j in range(val_rep):\n",
    "                rows_list.append(dataframe.iloc[[i]])\n",
    "     \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = pd.DataFrame(rows_list)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe.groupby('Department').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Department'].value_counts().plot(kind = 'hist', bins= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.plot.area(alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"Sentence\"] = dataframe['job_description'].astype(str) + ' ' + dataframe['Job_Department'].astype(str)  + ' '+ ' ' + dataframe['Other_Skills'].astype(str) + ' ' +  dataframe['Skills'].astype(str) + ' ' + dataframe['Keywords'].astype(str) +' ' + dataframe['job_title'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe[['Document ID','Sentence','Department']]\n",
    "#data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame()\n",
    "final_data = data.copy()\n",
    "final_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "\n",
    "    text=text.lower()\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = re.sub('#([^\\s]+)', '', text)\n",
    "    text = re.sub('[:;>?<=*+()&,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
    "    text = re.sub('[\\d]','', text)\n",
    "    text = text.replace(\".\", '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"'s\", '')\n",
    "    text = text.replace(\"/\", ' ')\n",
    "    text = text.replace(\"\\\"\", ' ')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = text.replace(\"nbsp\", '')\n",
    "    re.sub(' +', ' ', text)\n",
    "    text=text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    #normalize some utf8 encoding\n",
    "    text = text.replace(\"\\x9d\",'').replace(\"\\x8c\",'')\n",
    "    text = text.replace(\"\\xa0\",'')\n",
    "    text = text.replace(\"\\x9d\\x92\", '').replace(\"\\x9a\\xaa\\xf0\\x9f\\x94\\xb5\", '').replace(\"\\xf0\\x9f\\x91\\x8d\\x87\\xba\\xf0\\x9f\\x87\\xb8\", '').replace(\"\\x9f\",'').replace(\"\\x91\\x8d\",'')\n",
    "    text = text.replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\",'').replace(\"\\xf0\",'').replace('\\xf0x9f','').replace(\"\\x9f\\x91\\x8d\",'').replace(\"\\x87\\xba\\x87\\xb8\",'')\n",
    "    text = text.replace(\"\\xe2\\x80\\x94\",'').replace(\"\\x9d\\xa4\",'').replace(\"\\x96\\x91\",'').replace(\"\\xe1\\x91\\xac\\xc9\\x8c\\xce\\x90\\xc8\\xbb\\xef\\xbb\\x89\\xd4\\xbc\\xef\\xbb\\x89\\xc5\\xa0\\xc5\\xa0\\xc2\\xb8\",'')\n",
    "    text = text.replace(\"\\xe2\\x80\\x99s\", \"\").replace(\"\\xe2\\x80\\x98\", '').replace(\"\\xe2\\x80\\x99\", '').replace(\"\\xe2\\x80\\x9c\", \"\").replace(\"\\xe2\\x80\\x9d\", \"\")\n",
    "    text = text.replace(\"\\xe2\\x82\\xac\", \"\").replace(\"\\xc2\\xa3\", \"\").replace(\"\\xc2\\xa0\", \"\").replace(\"\\xc2\\xab\", \"\").replace(\"\\xf0\\x9f\\x94\\xb4\", \"\").replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\\xf0\\x9f\", \"\")\n",
    "    text =  re.sub(r\"\\b[a-z]\\b\", \"\", text)\n",
    "    text=re.sub( '\\s+', ' ', text).strip()\n",
    "    \n",
    "    text=re.sub(r'\\.+', \".\", text)\n",
    "    text=re.sub(r'\\.\\.+', ' ', text).replace('.', '')\n",
    "    # Replace multiple dots with space\n",
    "    text = re.sub('\\.\\.+', ' ', text) \n",
    "    # Remove single dots\n",
    "    text = re.sub('\\.', '', text)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'\\.{1}', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(final_data.shape[0]):\n",
    "    text =normalize_text(final_data.iloc[i]['Sentence'])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    newtokens=[]\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            newtokens.append(token)\n",
    "       \n",
    "    newtokens = set(newtokens)\n",
    "    newtokens = list(newtokens)\n",
    "    #print(newtokens)\n",
    "    sent = \" \".join(str(x) for x in newtokens)\n",
    "    sent = \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "             if w.lower() in words or not w.isalpha())\n",
    "    temp.append(sent)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['tokens'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classes = set(dataframe['Department'])\n",
    "data_classes = list(data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classes[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data['Department_index'] = final_data['Department'].apply(data_classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.drop('Sentence',axis = 1)\n",
    "final_data = final_data.drop('Document ID',axis = 1)\n",
    "final_data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Modelling & Evaluation\n",
    "\n",
    "_Include all model prepration & evaluation steps in this part._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql','php','ios','javascript','python','c','css','android','iphone','sql','objective-c','c++','angularjs','.net']\n",
    "my_tags = ['Administration','Airline Ground Operations', 'Analytics', 'Back office ticketing', 'Customer service', 'Data entry', 'Digital Marketing', 'Engineering Design Construction', 'Finance', 'Flight Operations', 'IT', 'Learning and Development ', 'Logistics', 'Maintenance', 'Management Consulting', 'Marine Deck ', 'Marine Engineering ', 'Marine Service Steward ', 'Marketing', 'Operations', 'Presales ', 'Public Relations ', 'QA ', 'Recruitment', 'Sales', 'Technology', 'Ticketing']\n",
    "#plt.figure(figsize=(10,4))\n",
    "print(len(my_tags))\n",
    "#df.tags.value_counts().plot(kind='bar');\n",
    "final_data.Department.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data.tokens\n",
    "y = final_data.Department\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"data_1/wiki-news-300d-1M.vec\", )\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(final_data, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['tokens']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['tokens']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['Department'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.Department))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec \n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(gensim.models.doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_data.tokens, final_data.Department, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(final_data) * .8)\n",
    "train_posts = final_data['tokens'][:train_size]\n",
    "train_tags = final_data['Department'][:train_size]\n",
    "\n",
    "test_posts = final_data['tokens'][train_size:]\n",
    "test_tags = final_data['Department'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "num_classes = np.max(y_train) + 1\n",
    "\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :CNN',fontsize=16)\n",
    "fig1.savefig('loss_cnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
